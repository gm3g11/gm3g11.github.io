<!DOCTYPE html>
<html lang="zh-cn">
  <head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  <meta name="author" content="Guangyu">
  
  
  
  <link rel="prev" href="https://gm3g11.github.io/2020/vgg-research/" />
  <link rel="next" href="https://gm3g11.github.io/2020/cyclegan/" />
  <link rel="canonical" href="https://gm3g11.github.io/2020/literature-review-on-esrgan-enhanced-super-resolution-generative-adversarial-networks/" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <title>
       
       
           Literature review on ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks | Guangyu&#39;s blog
       
  </title>
  <meta name="title" content="Literature review on ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks | Guangyu&#39;s blog">
    
  
  <link rel="stylesheet" href="/font/iconfont.css">
  <link rel="stylesheet" href="/css/main.min.css">


  
  
 

<script type="application/ld+json">
 "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "https:\/\/gm3g11.github.io\/"
    },
    "articleSection" : "posts",
    "name" : "Literature review on ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks",
    "headline" : "Literature review on ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks",
    "description" : "Literature review on \u0026ldquo;ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks\u0026rdquo; \u0026ldquo;https:\/\/arxiv.org\/pdf\/1809.00219.pdf\u0026quot;\n1.Abstract: To be solved issue： The hallucinated details are often accompanied with unpleasant artifacts\nMethod: In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN [2] to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery.",
    "inLanguage" : "zh-cn",
    "author" : "Guangyu",
    "creator" : "Guangyu",
    "publisher": "Guangyu",
    "accountablePerson" : "Guangyu",
    "copyrightHolder" : "Guangyu",
    "copyrightYear" : "2020",
    "datePublished": "2020-06-28 23:58:36 -0500 CDT",
    "dateModified" : "2020-06-28 23:58:36 -0500 CDT",
    "url" : "https:\/\/gm3g11.github.io\/2020\/literature-review-on-esrgan-enhanced-super-resolution-generative-adversarial-networks\/",
    "wordCount" : "679",
    "keywords" : [  "Guangyu\u0027s blog"]
}
</script>

</head>

  


  <body class="">
    <div class="wrapper">
        <nav class="navbar">
    <div class="container">
        <div class="navbar-header header-logo">
        	<a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://gm3g11.github.io/">Guangyu&#39;s blog</a>
        </div>
        <div class="menu navbar-right">
                
                
                <a class="menu-item" href="/posts/" title="">blog</a>
                
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
     <div class="container">
        <div class="navbar-header">
            <div>  <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://gm3g11.github.io/">Guangyu&#39;s blog</a></div>
            <div class="menu-toggle">
                <span></span><span></span><span></span>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/posts/" title="">blog</a>
                
        </div>
    </div>
</nav>
    	 <main class="main">
          <div class="container">
      		
<article class="post-warp" itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">Literature review on ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks</h1>
        <div class="post-meta">
                Written by <a itemprop="name" href="https://gm3g11.github.io/" rel="author">Guangyu</a> with ♥ 
                <span class="post-time">
                on <time datetime=2020-06-28 itemprop="datePublished">June 28, 2020</time>
                </span>
                in
                
        </div>
    </header>
    <div class="post-content">
        

        
            
        

        
        
     
          
          
          

          
          
          

          <h1 id="literature-review-on-esrgan-enhanced-super-resolution-generative-adversarial-networks">Literature review on &ldquo;ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks&rdquo;</h1>
<p>&ldquo;<a href="https://arxiv.org/pdf/1809.00219.pdf%22">https://arxiv.org/pdf/1809.00219.pdf&quot;</a></p>
<h2 id="1abstract">1.Abstract:</h2>
<h4 id="to-be-solved-issue">To be solved issue：</h4>
<p>The hallucinated details are often accompanied with unpleasant artifacts</p>
<h4 id="method">Method:</h4>
<p>In particular, we introduce the <strong>Residual-in-Residual Dense Block (RRDB) without batch normalization</strong> as the basic network building unit. Moreover, we borrow the idea from <strong>relativistic GAN</strong> [2] to let the discriminator predict relative realness instead of the absolute value. Finally, we <strong>improve the perceptual loss by using the features before activation</strong>, which could provide stronger supervision for brightness consistency and texture recovery.</p>
<h2 id="2introduction">2.Introduction:</h2>
<p>These <strong>PSNR-oriented approaches tend to output over-smoothed results</strong> without sufficient high-frequency details, since the PSNR metric fundamentally disagrees with the subjective evaluation of human observers [1]</p>
<p><strong>Perceptual loss</strong>  is proposed to optimize super-resolution model in a feature space instead of pixel space</p>
<p><strong>Generative adversarial network</strong> [15] is introduced to SR by [1,16] to encourage the network to favor solutions that look more like natural images.</p>
<p>First, we improve the network structure by introducing the <strong>Residual-in-Residual Dense Block (RDDB)</strong>, which is of higher capacity and easier to train. We also <strong>remove Batch Normalization (BN)</strong> [19] layers as in [20] and <strong>use residual scaling</strong> [21,20] and <strong>smaller initialization</strong> to facilitate training a very deep network. Second, we <strong>improve the discriminator using Relativistic average GAN</strong> (RaGAN) [2], which learns to judge “whether one image is more realistic than the other” rather than “whether one image is real or fake”. Our experiments show that this improvement helps the generator recover more realistic texture details. Third, we propose an improved perceptual loss by <strong>using the VGG features before activation</strong> instead of after activation as in SRGAN.</p>
<p><strong>perceptual loss</strong> [13] is proposed to enhance the visual quality by minimizing the error in a feature space instead of pixel space. <strong>Contextual loss</strong> [30] is developed to generate images with natural image statistics by using an objective that focuses on the feature distribution rather than merely comparing the appearance.</p>
<h2 id="3proposed-method">3.Proposed Method</h2>
<h4 id="31-network">3.1 Network</h4>
<p><img src="/photo/SRGAN/image-20200626132110960.png" alt="image-20200626132110960"></p>
<p><img src="/photo/SRGAN/image-20200626132119704.png" alt="image-20200626132119704"></p>
<p><em>RRDB:</em>
$$
Input:img\<br>
img_1=img+\beta f(img)\<br>
img_2=img_1+\beta f(img_1)\<br>
img_3=img_2+\beta f(img_2)\<br>
img_{out}=img+\beta f(img_3)
$$
*Dense Block:*
$$
Input: img\<br>
img_1=img+f(img)\<br>
img_2=img_1+img+ f(img_1)\<br>
img_3=img_2+img_1+img+ f(img_2)\<br>
img_{4}=img_3+img_2+img_1+img+ f(img_3)
$$</p>
<h4 id="32-relativistic-discriminator">3.2 Relativistic Discriminator</h4>
<p><img src="/photo/SRGAN/image-20200626133320221.png" alt="image-20200626133320221"></p>
<p>The discriminator loss is then defined as:</p>
<p><img src="/photo/SRGAN/image-20200626133821644.png" alt="image-20200626133821644"></p>
<p><img src="/photo/SRGAN/image-20200626133851144.png" alt="image-20200626133851144"></p>
<p>$D_{Ra}$ is short for Relativistic average Discriminator . $E_{x_f}$ is  taking average for all fake data in the mini-batch.</p>
<p>$x_r$ is real image, $x_f=G(x_i)$  $x_i$ is LR image.</p>
<h4 id="33-perceptual-loss">3.3 Perceptual Loss</h4>
<p>We propose to use features before the activation layers, which will overcome <strong>two drawbacks</strong> of the original design. First, the <strong>activated features are very sparse</strong>, especially after a very deep network. The sparse activation provides weak supervision and thus leads to inferior performance. Second, using features after activation also <strong>causes inconsistent reconstructed brightness</strong> compared with the ground-truth image</p>
<p><img src="/photo/SRGAN/image-20200626140252617.png" alt="image-20200626140252617"></p>
<p><img src="/photo/SRGAN/image-20200626140313822.png" alt="image-20200626140313822"></p>
<h2 id="4-experiments">4 Experiments</h2>
<h4 id="41-training-details">4.1 Training Details</h4>
<p><strong>The training process is divided into two stages</strong>. <strong>First,</strong> we train a PSNR-oriented model with the L1 loss. We <strong>then</strong> employ the trained PSNR-oriented model as an initialization for the generator. <strong>The reasons</strong> are that 1) it can avoid undesired local optima for the generator; 2) after pre-training, the discriminator receives relatively good super-resolved images instead of extreme fake ones</p>
<h4 id="44-ablation-study">4.4 Ablation Study</h4>
<p><strong>BN removal:</strong> We first remove all BN layers for stable and consistent performance without artifacts. It does not decrease the performance but saves the computational resources and memory usage</p>
<p><strong>Before activation in perceptual loss:</strong> We first demonstrate that using features before activation can result in more accurate brightness of reconstructed images.</p>
<p><strong>RaGAN:</strong>  RaGAN uses an improved relativistic discriminator, which is shown to benefit learning sharper edges and more detailed textures</p>
<p><strong>Deeper network with RRDB:</strong> Deeper model with the proposed RRDB can further improve the recovered textures, especially for the regular structures</p>
<p>[1] Ledig, C., Theis, L., Husz´ar, F., Caballero, J., Cunningham, A., Acosta, A., Aitken, A., Tejani, A., Totz, J., Wang, Z., et al.: Photo-realistic single image superresolution using a generative adversarial network. In: CVPR. (2017)</p>
<p>[2] Jolicoeur-Martineau, A.: The relativistic discriminator: a key element missing from standard gan. arXiv preprint arXiv:1807.00734 (2018)</p>
<p>[3] Blau, Y., Mechrez, R., Timofte, R., Michaeli, T., Zelnik-Manor, L.: The pirm challenge on perceptual super resolution.&rdquo; <a href="https://www.pirm2018.org/PIRM-SR">https://www.pirm2018.org/PIRM-SR</a>. html &ldquo;(2018)</p>

    </div>

    <div class="post-copyright">
             
            <p class="copyright-item">
                <span>Author:</span>
                <span>Guangyu </span>
                </p>
            
           
             
            <p class="copyright-item">
                    <span>Link:</span>
                    <a href=https://gm3g11.github.io/2020/literature-review-on-esrgan-enhanced-super-resolution-generative-adversarial-networks/>https://gm3g11.github.io/2020/literature-review-on-esrgan-enhanced-super-resolution-generative-adversarial-networks/</span>
            </p>
            
             
            <p class="copyright-item lincese">
                @copyright Guangyu Meng
            </p>
            
    </div>

  
    <div class="post-tags">
        
        <section>
                <a href="javascript:window.history.back();">back</a></span> · 
                <span><a href="https://gm3g11.github.io/">home</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="https://gm3g11.github.io/2020/vgg-research/" class="prev" rel="prev" title="VGG Research"><i class="iconfont icon-left"></i>&nbsp;VGG Research</a>
         
        
        <a href="https://gm3g11.github.io/2020/cyclegan/" class="next" rel="next" title="Literature review on Cycle GAN">Literature review on Cycle GAN&nbsp;<i class="iconfont icon-right"></i></a>
        
    </div>

    <div class="post-comment">
          
                 
          
    </div>
</article>
          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span itemprop="copyrightYear">2020 - 2020</span>
        
        <span class="with-love">
    	 <i class="iconfont icon-love"></i> 
         </span>
         
            <span class="author" itemprop="copyrightHolder"><a href="https://gm3g11.github.io/">Guangyu</a> | </span> 
         

         
		  <span>Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow">Hugo</a> & <a href="https://github.com/liuzc/leaveit" target="_blank" rel="external nofollow">LeaveIt</a></span> 
    </div>
</footer>












    
     <link href="//lib.baomitu.com/lightgallery/1.6.11/css/lightgallery.min.css" rel="stylesheet">  
      
     <script src="/js/vendor_gallery.min.js" async="" ></script>
    
  



     </div>
  </body>
</html>
